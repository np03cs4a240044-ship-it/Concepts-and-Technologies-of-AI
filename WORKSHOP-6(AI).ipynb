{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##SIGMOD REGRESSION\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KdwRczU3qZqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "BwHyCWNsrQ9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "n55bBR_Rr5MC",
        "outputId": "ab1cdc7f-91a9-4b8f-9c28-76e37597ac5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ef3518c-ced8-4b5c-b8a4-aefeedb024a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5ef3518c-ced8-4b5c-b8a4-aefeedb024a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Iris.csv to Iris.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 1"
      ],
      "metadata": {
        "id": "Hc-0GIS6qe0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_function(x):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "       x: scalar or numpy array of any size.\n",
        "    Returns:\n",
        "       y: logistic function applied to x.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y"
      ],
      "metadata": {
        "id": "20q8kz4tqd-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test\n"
      ],
      "metadata": {
        "id": "90QhgM-SsO8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function with the provided test case\n",
        "def test_logistic_function():\n",
        "    \"\"\"\n",
        "    Test cases for the logistic_function.\n",
        "    \"\"\"\n",
        "    # Test with scalar input\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)  # Expected output: 0.5\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "\n",
        "    # Test with positive scalar input\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)  # Expected output: ~0.881\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "\n",
        "    # Test with negative scalar input\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)  # Expected output: ~0.047\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "\n",
        "    # Test with numpy array input\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])  # Adjusted expected values rounded to 3 decimals\n",
        "    # Use np.round to round the array element-wise and compare\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "# Run the test case\n",
        "test_logistic_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhmE875JsOKE",
        "outputId": "5c2cf54d-ee4f-4beb-ee99-5ce95f4f0da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 2\n"
      ],
      "metadata": {
        "id": "kdeNkjldtNoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "       y_true (scalar): true target value {0 or 1}.\n",
        "       y_pred (scalar): predicted target value {0-1}.\n",
        "    Returns:\n",
        "       loss (float): loss/error value\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    # Ensure y_pred is clipped to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "\n",
        "    # Log loss formula: L(y, ŷ) = -y * log(ŷ) - (1-y) * log(1-ŷ)\n",
        "    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "02IgRjjfsVLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test\n"
      ],
      "metadata": {
        "id": "cxc7qJAYtVD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "    \"\"\"\n",
        "    Test cases for the log_loss function.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "    y_true = 1\n",
        "    y_pred = 1\n",
        "    expected_loss = 0.0  # Log loss is 0 for perfect prediction\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=1, y_pred=1)\"\n",
        "\n",
        "    # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "    y_true = 0\n",
        "    y_pred = 0\n",
        "    expected_loss = 0.0  # Log loss is 0 for perfect prediction\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=0, y_pred=0)\"\n",
        "\n",
        "    # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
        "    y_true = 1\n",
        "    y_pred = 0\n",
        "    try:\n",
        "        log_loss(y_true, y_pred)  # This should raise an error due to log(0)\n",
        "    except ValueError:\n",
        "        pass  # Test passed if ValueError is raised for log(0)\n",
        "\n",
        "    # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "    y_true = 0\n",
        "    y_pred = 1\n",
        "    try:\n",
        "        log_loss(y_true, y_pred)  # This should raise an error due to log(0)\n",
        "    except ValueError:\n",
        "        pass  # Test passed if ValueError is raised for log(0)\n",
        "\n",
        "    # Test case 5: Partially correct prediction\n",
        "    y_true = 1\n",
        "    y_pred = 0.8\n",
        "    expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2))  # ~0.2231\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=1, y_pred=0.8)\"\n",
        "\n",
        "    y_true = 0\n",
        "    y_pred = 0.2\n",
        "    expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8))  # ~0.2231\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "# Run the test case\n",
        "test_log_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErzAymzAtUQf",
        "outputId": "b88a79e3-39a4-4473-e200-fe469fba73d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 3"
      ],
      "metadata": {
        "id": "m5aWrosftug4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "    Args:\n",
        "        y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
        "        y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
        "    Returns:\n",
        "        cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "\n",
        "    n = len(y_true)\n",
        "\n",
        "    # Calculate loss for each observation and sum them up\n",
        "    total_loss = 0\n",
        "    for i in range(n):\n",
        "        total_loss += log_loss(y_true[i], y_pred[i])\n",
        "\n",
        "    # Average loss\n",
        "    cost = total_loss / n\n",
        "    return cost="
      ],
      "metadata": {
        "id": "l8jfAnintbTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the cost function\n",
        "import numpy as np\n",
        "\n",
        "def test_cost_function():\n",
        "    # Test case 1: Simple example with known expected cost\n",
        "    y_true = np.array([1, 0, 1])\n",
        "    y_pred = np.array([0.9, 0.1, 0.8])\n",
        "\n",
        "    # Expected output: Manually calculate cost for these values\n",
        "    # log_loss(y_true, y_pred) for each example\n",
        "    expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
        "                     -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
        "                     -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
        "\n",
        "    # Call the cost_function to get the result\n",
        "    result = cost_function(y_true, y_pred)\n",
        "\n",
        "    # Assert that the result is close to the expected cost with a tolerance of 1e-6\n",
        "    assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "\n",
        "    print(\"Test passed for simple case!\")\n",
        "\n",
        "# Run the test case\n",
        "test_cost_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SofusThHuOSd",
        "outputId": "01e52eae-c36c-4598-f298-5c241a154a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed for simple case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 4"
      ],
      "metadata": {
        "id": "74bzpw7_uadQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def costfunction_logreg(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        X (ndarray, shape (m,n)): data on features, m observations with n features.\n",
        "        y (array_like, shape (m,)): array of true values of target (0 or 1).\n",
        "        w (array_like, shape (n,)): weight parameters of the model.\n",
        "        b (float): bias parameter of the model.\n",
        "\n",
        "    Returns:\n",
        "        cost (float): nonnegative cost corresponding to y and y_pred.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
        "    assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
        "\n",
        "    # Compute z = X * w + b (linear combination)\n",
        "    z = np.dot(X, w) + b\n",
        "\n",
        "    # Compute predictions using logistic function (sigmoid)\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    # Compute the cost using the cost_function we already implemented\n",
        "    cost = cost_function(y, y_pred)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "4uC9TlWXuQTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the costfunction_logreg\n",
        "print(\"Testing costfunction_logreg:\")\n",
        "\n",
        "# Test case from the worksheet\n",
        "X = np.array([[10, 20], [-10, 10]])  # shape (2, 2)\n",
        "y = np.array([1, 0])  # shape (2,)\n",
        "w = np.array([0.5, 1.5])  # shape (2,)\n",
        "b = 1  # scalar\n",
        "\n",
        "# Calculate cost\n",
        "cost = costfunction_logreg(X, y, w, b)\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {cost}\")\n",
        "\n",
        "# Let's manually verify this step by step:\n",
        "print(\"\\nStep-by-step verification:\")\n",
        "print(f\"1. X shape: {X.shape}, y shape: {y.shape}, w shape: {w.shape}\")\n",
        "print(f\"2. Compute z = X * w + b:\")\n",
        "z = np.dot(X, w) + b\n",
        "print(f\"   z = {z}\")\n",
        "print(f\"3. Apply sigmoid to get predictions:\")\n",
        "y_pred = logistic_function(z)\n",
        "print(f\"   y_pred = {y_pred}\")\n",
        "print(f\"4. Calculate cost using cost_function:\")\n",
        "manual_cost = cost_function(y, y_pred)\n",
        "print(f\"   cost = {manual_cost}\")\n",
        "print(f\"5. Our function returned: {cost}\")\n",
        "print(f\"   Match: {np.isclose(cost, manual_cost)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcHO5fb2ufRT",
        "outputId": "e2d321d4-6289-4828-d19b-b866c8df6dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing costfunction_logreg:\n",
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n",
            "\n",
            "Step-by-step verification:\n",
            "1. X shape: (2, 2), y shape: (2,), w shape: (2,)\n",
            "2. Compute z = X * w + b:\n",
            "   z = [36. 11.]\n",
            "3. Apply sigmoid to get predictions:\n",
            "   y_pred = [1.        0.9999833]\n",
            "4. Calculate cost using cost_function:\n",
            "   cost = 5.500008350834906\n",
            "5. Our function returned: 5.500008350834906\n",
            "   Match: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 5"
      ],
      "metadata": {
        "id": "H88eNMnOuu3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes gradients of the cost function with respect to model parameters.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray, shape (n,d)): Input data, n observations with d features\n",
        "        y (array_like, shape (n,)): True labels (0 or 1)\n",
        "        w (array_like, shape (d,)): Weight parameters of the model\n",
        "        b (float): Bias parameter of the model\n",
        "\n",
        "    Returns:\n",
        "        grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight parameters\n",
        "        grad_b (float): Gradient of the cost function with respect to the bias parameter\n",
        "    \"\"\"\n",
        "    n, d = X.shape  # X has shape (n, d)\n",
        "    assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "    assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "\n",
        "    # Compute predictions using logistic function (sigmoid)\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    # Compute gradients\n",
        "    # Formula: ∂L/∂w = (1/n) * X^T * (y_pred - y)\n",
        "    # Formula: ∂L/∂b = (1/n) * sum(y_pred - y)\n",
        "    error = y_pred - y\n",
        "    grad_w = np.dot(X.T, error) / n  # Gradient w.r.t weights, shape (d,)\n",
        "    grad_b = np.sum(error) / n  # Gradient w.r.t bias, scalar\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "metadata": {
        "id": "6XlLNluXugz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple test case from the worksheet\n",
        "X = np.array([[10, 20], [-10, 10]]) # shape (2, 2)\n",
        "y = np.array([1, 0]) # shape (2,)\n",
        "w = np.array([0.5, 1.5]) # shape (2,)\n",
        "b = 1 # scalar\n",
        "\n",
        "# Assertion tests\n",
        "try:\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "    print(\"Gradients computed successfully.\")\n",
        "    print(f\"grad_w: {grad_w}\")\n",
        "    print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "    print(f\"Assertion error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKRz9Y_Euwtk",
        "outputId": "43786fd7-6fb8-4cd0-cdab-3c96ef8db948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [-4.99991649  4.99991649]\n",
            "grad_b: 0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To Do 6"
      ],
      "metadata": {
        "id": "_Gwmj2E-vG5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "    \"\"\"\n",
        "    Implements batch gradient descent to optimize logistic regression parameters.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
        "        y (array_like, shape (n,)): True values of target (0 or 1)\n",
        "        w (array_like, shape (d,)): Initial weight parameters\n",
        "        b (float): Initial bias parameter\n",
        "        alpha (float): Learning rate\n",
        "        n_iter (int): Number of iterations\n",
        "        show_cost (bool): If True, displays cost every 100 iterations\n",
        "        show_params (bool): If True, displays parameters every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "        w (array_like, shape (d,)): Optimized weight parameters\n",
        "        b (float): Optimized bias parameter\n",
        "        cost_history (list): List of cost values over iterations\n",
        "        params_history (list): List of parameters (w, b) over iterations\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "    assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "\n",
        "        # Update weights and bias\n",
        "        w -= alpha * grad_w\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        # Compute cost\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "\n",
        "        # Store cost and parameters\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "        # Optionally print cost and parameters\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "        if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "\n",
        "    return w, b, cost_history, params_history"
      ],
      "metadata": {
        "id": "nszobto8u-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple assertion test for gradient_descent\n",
        "def test_gradient_descent():\n",
        "    X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "    y = np.array([1, 0]) # Shape (2,)\n",
        "    w = np.zeros(X.shape[1]) # Shape (2,)\n",
        "    b = 0.0 # Scalar\n",
        "    alpha = 0.1 # Learning rate\n",
        "    n_iter = 100 # Number of iterations\n",
        "\n",
        "    # Run gradient descent\n",
        "    w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=False)\n",
        "\n",
        "    # Assertions\n",
        "    assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "    assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "    assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "    assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_gradient_descent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peBZ8KGzvJ62",
        "outputId": "1dad1a03-24f2-4e34-a63e-107caf7a8305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 7"
      ],
      "metadata": {
        "id": "Y0f2LBK9vkK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(X, w, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predicts binary outcomes for given input features based on logistic regression parameters.\n",
        "\n",
        "    Arguments:\n",
        "        X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d features.\n",
        "        w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
        "        b (float): Bias learned via gradient descent.\n",
        "        threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
        "\n",
        "    Returns:\n",
        "        y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
        "    \"\"\"\n",
        "    # Compute the predicted probabilities using the logistic function\n",
        "    z = np.dot(X, w) + b\n",
        "    y_test_prob = logistic_function(z)\n",
        "\n",
        "    # Classify based on the threshold\n",
        "    # If probability >= threshold, predict class 1, else predict class 0\n",
        "    y_pred = (y_test_prob >= threshold).astype(int)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "GJtV__7IvYAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple assertion test for Prediction Function\n",
        "def test_prediction():\n",
        "    X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]]) # Shape (3, 2)\n",
        "    w_test = np.array([1.0, -1.0]) # Shape (2,)\n",
        "    b_test = 0.0 # Scalar bias\n",
        "    threshold = 0.5 # Default threshold\n",
        "\n",
        "    # Updated expected output\n",
        "    expected_output = np.array([0, 1, 1])\n",
        "\n",
        "    # Call the prediction function\n",
        "    y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "\n",
        "    # Assert that the output matches the expected output\n",
        "    assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVrguKw0vl5i",
        "outputId": "290da635-ac24-498e-c0cb-6ef73cdad562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Softmax Regression"
      ],
      "metadata": {
        "id": "cAvu_tmAv3TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 1 Implement the Softmax Function"
      ],
      "metadata": {
        "id": "UB1O-y7Jv601"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Compute the softmax of a 2D numpy array along the specified axis.\n",
        "\n",
        "    Parameters:\n",
        "        z (numpy.ndarray): Input array of shape (m, n) where m is the number of samples and n is the number of classes.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Softmax probabilities of the same shape as input (m, n), where each row sums to 1 and\n",
        "                       represents the probability distribution over classes for a sample.\n",
        "\n",
        "    Notes:\n",
        "        - Applies a normalization trick to prevent numerical instability by subtracting the max value in each row before exponentiation.\n",
        "    \"\"\"\n",
        "    # Normalize input to prevent numerical instability\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "6J8b4z_fvnnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Cases for Softmax Function\n",
        "def test_softmax():\n",
        "    \"\"\"\n",
        "    Perform basic assertion tests on the softmax function to validate its correctness.\n",
        "\n",
        "    Tests:\n",
        "    - Ensure that the output probabilities sum to 1 for each row.\n",
        "    - Ensure non-negative values (all probabilities should be >= 0).\n",
        "    - Test on edge cases (e.g., all zeros, very large or small values).\n",
        "    \"\"\"\n",
        "\n",
        "    # Test input\n",
        "    test_cases = [\n",
        "        (np.array([[0, 0, 0]]), \"All zeros\"),\n",
        "        (np.array([[1, 2, 3]]), \"Simple case\"),\n",
        "        (np.array([[1000, 1000, 1000]]), \"Large identical values\"),\n",
        "        (np.array([[-1000, -1000, -1000]]), \"Small identical values\"),\n",
        "        (np.array([[1, 0, -1]]), \"Mixed positive and negative\")\n",
        "    ]\n",
        "\n",
        "    for i, (z, description) in enumerate(test_cases):\n",
        "        print(f\"Test {i + 1}: {description}\")\n",
        "        result = softmax(z)\n",
        "\n",
        "        # Check that probabilities sum to 1\n",
        "        assert np.allclose(result.sum(axis=1), 1), f\"Failed: Probabilities do not sum to 1 in {description}\"\n",
        "\n",
        "        # Check non-negativity\n",
        "        assert np.all(result >= 0), f\"Failed: Negative probabilities in {description}\"\n",
        "\n",
        "        print(f\"  Input: {z}\")\n",
        "        print(f\"  Output: {result}\")\n",
        "        print(f\"  Sum of row: {result.sum(axis=1)[0]:.6f}\")\n",
        "        print(\"  Passed.\")\n",
        "\n",
        "    print(\"\\nAll tests passed for softmax function!\")\n",
        "\n",
        "# Run the test\n",
        "test_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX2u75-8wM7H",
        "outputId": "c5397165-88c2-4184-ff01-87f6ce3632fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: All zeros\n",
            "  Input: [[0 0 0]]\n",
            "  Output: [[0.33333333 0.33333333 0.33333333]]\n",
            "  Sum of row: 1.000000\n",
            "  Passed.\n",
            "Test 2: Simple case\n",
            "  Input: [[1 2 3]]\n",
            "  Output: [[0.09003057 0.24472847 0.66524096]]\n",
            "  Sum of row: 1.000000\n",
            "  Passed.\n",
            "Test 3: Large identical values\n",
            "  Input: [[1000 1000 1000]]\n",
            "  Output: [[0.33333333 0.33333333 0.33333333]]\n",
            "  Sum of row: 1.000000\n",
            "  Passed.\n",
            "Test 4: Small identical values\n",
            "  Input: [[-1000 -1000 -1000]]\n",
            "  Output: [[0.33333333 0.33333333 0.33333333]]\n",
            "  Sum of row: 1.000000\n",
            "  Passed.\n",
            "Test 5: Mixed positive and negative\n",
            "  Input: [[ 1  0 -1]]\n",
            "  Output: [[0.66524096 0.24472847 0.09003057]]\n",
            "  Sum of row: 1.000000\n",
            "  Passed.\n",
            "\n",
            "All tests passed for softmax function!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 2 Implement the Loss Function for Softmax"
      ],
      "metadata": {
        "id": "gal8PQMbwRzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function for a Single Observation\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss for a single observation.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (numpy.ndarray): True labels (one-hot encoded) of shape (c,).\n",
        "        y_pred (numpy.ndarray): Predicted probabilities of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "        float: Cross-entropy loss for the observation.\n",
        "    \"\"\"\n",
        "    # Add epsilon to prevent log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))"
      ],
      "metadata": {
        "id": "Ho_PXv4twQcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Cases for Categorical Log-Loss Function\n",
        "def test_loss_softmax():\n",
        "    \"\"\"\n",
        "    Test the loss_softmax function using a known input and output.\n",
        "    \"\"\"\n",
        "    # Test Case 1: Perfect prediction\n",
        "    y_true = np.array([0, 1, 0])  # True label (one-hot encoded)\n",
        "    y_pred = np.array([0.1, 0.8, 0.1])  # Predicted probabilities\n",
        "    expected_loss = -np.log(0.8)  # Expected loss for perfect prediction\n",
        "    assert np.isclose(loss_softmax(y_true, y_pred), expected_loss), \"Test Case 1 Failed\"\n",
        "\n",
        "    # Test Case 2: Incorrect prediction\n",
        "    y_true = np.array([1, 0, 0])  # True label (one-hot encoded)\n",
        "    y_pred = np.array([0.3, 0.4, 0.3])  # Predicted probabilities\n",
        "    expected_loss = -np.log(0.3)  # Expected loss for incorrect prediction\n",
        "    assert np.isclose(loss_softmax(y_true, y_pred), expected_loss), \"Test Case 2 Failed\"\n",
        "\n",
        "    # Test Case 3: Edge case with near-zero probability\n",
        "    y_true = np.array([0, 1, 0])  # True label (one-hot encoded)\n",
        "    y_pred = np.array([0.01, 0.98, 0.01])  # Predicted probabilities\n",
        "    expected_loss = -np.log(0.98)  # Expected loss for edge case\n",
        "    assert np.isclose(loss_softmax(y_true, y_pred), expected_loss), \"Test Case 3 Failed\"\n",
        "\n",
        "    print(\"All test cases passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_loss_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKHygLr7wdBZ",
        "outputId": "ca96de68-2e87-46d4-efed-eb1dc81d5f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 3 Implement the Cost Function for Softmax"
      ],
      "metadata": {
        "id": "8CNP27qcwnLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost Function for Softmax (Average Loss)\n",
        "def cost_softmax(X, y, W, b):\n",
        "    \"\"\"\n",
        "    Compute the average cross-entropy cost over all samples.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "        y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "        W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "        b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "        float: Average cross-entropy cost over all samples.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    # Compute linear combination: z = X * W + b\n",
        "    z = np.dot(X, W) + b\n",
        "\n",
        "    # Compute predictions using softmax\n",
        "    y_pred = softmax(z)\n",
        "\n",
        "    # Compute average cross-entropy loss\n",
        "    # Add epsilon to prevent log(0)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / n"
      ],
      "metadata": {
        "id": "1OArff4Owebv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the Cost Function\n",
        "def test_cost_softmax():\n",
        "    \"\"\"\n",
        "    Test the cost_softmax function using a known input and output.\n",
        "    \"\"\"\n",
        "    # Test Case 1: Small dataset with perfect predictions\n",
        "    X = np.array([[1, 2], [2, 3], [3, 4]])  # Feature matrix (n=3, d=2)\n",
        "    y = np.array([[1, 0], [0, 1], [1, 0]])  # True labels (n=3, c=2, one-hot encoded)\n",
        "    W = np.array([[1, -1], [-1, 1]])  # Weight matrix (d=2, c=2)\n",
        "    b = np.array([0, 0])  # Bias vector (c=2)\n",
        "\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)  # Predicted probabilities\n",
        "\n",
        "    expected_cost = -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0]  # Compute expected cost\n",
        "    assert np.isclose(cost_softmax(X, y, W, b), expected_cost), \"Test Case 1 Failed\"\n",
        "\n",
        "    # Test Case 2: All-zero weights and bias\n",
        "    X = np.array([[1, 0], [0, 1], [1, 1]])  # Feature matrix (n=3, d=2)\n",
        "    y = np.array([[1, 0], [0, 1], [1, 0]])  # True labels (n=3, c=2, one-hot encoded)\n",
        "    W = np.zeros((2, 2))  # Zero weight matrix\n",
        "    b = np.zeros(2)  # Zero bias vector\n",
        "\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)  # Predicted probabilities (uniform distribution)\n",
        "\n",
        "    expected_cost = -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0]  # Compute expected cost\n",
        "    assert np.isclose(cost_softmax(X, y, W, b), expected_cost), \"Test Case 2 Failed\"\n",
        "\n",
        "    print(\"All test cases passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_cost_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40nSVLWzwpjt",
        "outputId": "476479bd-ce8e-4238-bede-d9c413219bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 4 Implement the Gradient Computation for Softmax"
      ],
      "metadata": {
        "id": "K4a1hEM7wwYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Computation for Softmax Regression\n",
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    \"\"\"\n",
        "    Compute the gradients of the cost function with respect to weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "        y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "        W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "        b (numpy.ndarray): Bias vector of shape (c,).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Gradients with respect to weights (d, c) and biases (c,).\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "\n",
        "    # Forward pass\n",
        "    z = np.dot(X, W) + b          # Linear combination: shape (n, c)\n",
        "    y_pred = softmax(z)           # Predictions: shape (n, c)\n",
        "\n",
        "    # Backward pass (gradients)\n",
        "    # Formula: grad_W = (1/n) * X^T * (y_pred - y)\n",
        "    # Formula: grad_b = (1/n) * sum(y_pred - y) across samples\n",
        "    error = y_pred - y            # Shape (n, c)\n",
        "    grad_W = np.dot(X.T, error) / n  # Shape (d, c)\n",
        "    grad_b = np.sum(error, axis=0) / n  # Shape (c,)\n",
        "\n",
        "    return grad_W, grad_b"
      ],
      "metadata": {
        "id": "WTsleDdBwrGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function for compute_gradient_softmax\n",
        "def test_compute_gradient_softmax():\n",
        "    # Define simple inputs\n",
        "    X = np.array([[1, 2], [3, 4]])  # Shape (2, 2)\n",
        "    y = np.array([[1, 0], [0, 1]])  # Shape (2, 2), one-hot encoded\n",
        "    W = np.array([[0.1, 0.2], [0.3, 0.4]])  # Shape (2, 2)\n",
        "    b = np.array([0.01, 0.02])  # Shape (2,)\n",
        "\n",
        "    # Expected gradients (calculated manually or using a reference implementation)\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "\n",
        "    # Manual calculation of expected gradients\n",
        "    error = y_pred - y\n",
        "    grad_W_expected = np.dot(X.T, error) / X.shape[0]\n",
        "    grad_b_expected = np.sum(error, axis=0) / X.shape[0]\n",
        "\n",
        "    # Compute gradients using the function\n",
        "    grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
        "\n",
        "    # Assertions\n",
        "    assert np.allclose(grad_W, grad_W_expected, atol=1e-6), \"Gradient W does not match expected values\"\n",
        "    assert np.allclose(grad_b, grad_b_expected, atol=1e-6), \"Gradient b does not match expected values\"\n",
        "\n",
        "    print(\"All tests passed for compute_gradient_softmax!\")\n",
        "\n",
        "    # Print results for verification\n",
        "    print(f\"\\nInput shapes: X{X.shape}, y{y.shape}, W{W.shape}, b{b.shape}\")\n",
        "    print(f\"Computed grad_W shape: {grad_W.shape}\")\n",
        "    print(f\"Computed grad_b shape: {grad_b.shape}\")\n",
        "\n",
        "# Run the test\n",
        "test_compute_gradient_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y9bEsGVwyzv",
        "outputId": "171d76d8-9936-4ad1-f0ef-c56c122bca27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed for compute_gradient_softmax!\n",
            "\n",
            "Input shapes: X(2, 2), y(2, 2), W(2, 2), b(2,)\n",
            "Computed grad_W shape: (2, 2)\n",
            "Computed grad_b shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do 5 Implement Gradient Descent for Softmax"
      ],
      "metadata": {
        "id": "_cZkM74Cw-9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent for Softmax Regression\n",
        "def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the weights and biases.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "        y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "        W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "        b (numpy.ndarray): Bias vector of shape (c,).\n",
        "        alpha (float): Learning rate.\n",
        "        n_iter (int): Number of iterations.\n",
        "        show_cost (bool): Whether to display the cost at intervals.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Optimized weights, biases, and cost history.\n",
        "    \"\"\"\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
        "\n",
        "        # Update parameters (move opposite to gradient)\n",
        "        W -= alpha * grad_W\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        # Compute and store cost\n",
        "        cost = cost_softmax(X, y, W, b)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Optionally print progress\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "\n",
        "    return W, b, cost_history"
      ],
      "metadata": {
        "id": "lxVu1EpJw1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for gradient_descent_softmax with plot\n",
        "def test_gradient_descent_softmax_with_plot():\n",
        "    # Generate synthetic data for testing\n",
        "    np.random.seed(0)\n",
        "    n, d, c = 100, 5, 3  # 100 samples, 5 features, 3 classes\n",
        "    X = np.random.rand(n, d)\n",
        "    y_indices = np.random.randint(0, c, size=n)\n",
        "    y = np.zeros((n, c))\n",
        "    y[np.arange(n), y_indices] = 1  # One-hot encoding\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    W = np.random.randn(d, c) * 0.01  # Small random weights\n",
        "    b = np.random.randn(c) * 0.01     # Small random biases\n",
        "\n",
        "    # Parameters for gradient descent\n",
        "    alpha = 0.01  # Learning rate\n",
        "    n_iter = 500  # Number of iterations\n",
        "\n",
        "    print(\"Running gradient descent for softmax regression...\")\n",
        "    print(f\"Data: {n} samples, {d} features, {c} classes\")\n",
        "    print(f\"Initial cost: {cost_softmax(X, y, W, b):.6f}\")\n",
        "\n",
        "    # Run gradient descent\n",
        "    W_opt, b_opt, cost_history = gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False)\n",
        "\n",
        "    # Plot the cost history\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(n_iter), cost_history, label=\"Cost\", color='blue', linewidth=2)\n",
        "    plt.xlabel(\"Iteration\", fontsize=12)\n",
        "    plt.ylabel(\"Cost\", fontsize=12)\n",
        "    plt.title(\"Cost Reduction Over Iterations (Learning Rate: {alpha})\", fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Final cost should ideally be less than initial cost\n",
        "    print(f\"\\nInitial Cost: {cost_history[0]:.6f}\")\n",
        "    print(f\"Final Cost: {cost_history[-1]:.6f}\")\n",
        "    print(f\"Cost reduction: {((cost_history[0] - cost_history[-1]) / cost_history[0] * 100):.2f}%\")\n",
        "\n",
        "# Run the test\n",
        "test_gradient_descent_softmax_with_plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "mRWBQHopxBZy",
        "outputId": "0375bb07-f21f-40ce-bcd5-a190345ddaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running gradient descent for softmax regression...\n",
            "Data: 100 samples, 5 features, 3 classes\n",
            "Initial cost: 1.098303\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial Cost: 1.098262\n",
            "Final Cost: 1.089395\n",
            "Cost reduction: 0.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-A08Q35bxH1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
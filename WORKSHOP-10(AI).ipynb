{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1:**\n",
        "1. Load the dataset and preprocess the reviews.\n",
        "\n",
        "a. Convert all text to lowercase.\n",
        "\n",
        "b. Remove non-alphabetic characters (punctuation).\n",
        "\n",
        "c. Tokenize the reviews and remove common stopwords.\n",
        "\n",
        "d. Apply stemming to reduce words to their root form.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aj5wR1niQScb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPBfw7ryNvqX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/MyDrive/Concepts and technologies of AI/IMDB Dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "w_5WVmUCj3uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords and apply stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['clean_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "print(df[['review','clean_review','sentiment']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bHU56P1Q7di",
        "outputId": "a90f50ac-2507-47a3-f56c-e56937c89684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Split the dataset into training and testing sets (80% training, 20% testing).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xVjEU8LAR4OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['clean_review']\n",
        "y = df['sentiment'].map({'positive':1, 'negative':0})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(len(X_train), len(X_test))"
      ],
      "metadata": {
        "id": "8J6s9G0wRpGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use a Naive Bayes classifier to classify the reviews into positive and negative categories.\n",
        "\n",
        "a. Implement a Bag-of-Words model using CountVectorizer.\n",
        "\n",
        "b. Train the Naive Bayes classifier using the training set.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5guIeDvyR7Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_bow, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test_bow)"
      ],
      "metadata": {
        "id": "mraBdDb8Rx0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2:**\n",
        "1. Evaluate the performance of the model using the following metrics:\n",
        "\n",
        "a. Accuracy\n",
        "\n",
        "b. Precision, Recall, and F1-score\n",
        "\n",
        "c. Confusion Matrix\n",
        "\n",
        "d. ROC-AUC Score\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eaisg2sYSGvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "fOwNzEa_SLTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1 Feature Selection using Wrapper Methods.**\n",
        "**Part 1: Data Loading and Preprocessing**\n",
        "\n",
        "1. Load the Breast Cancer Prognostic Dataset.\n",
        "2. Dataset is available in Drive.\n",
        "3. Perform basic exploratory data analysis (EDA) to understand the dataset:\n",
        "\n",
        "• Summarize key statistics for each feature.\n",
        "\n",
        "• Check for missing values and handle them appropriately.\n",
        "\n",
        "4. Split the dataset into training (80%) and testing (20%) sets.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qi3im-XuSf9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/MyDrive/Concept and technologies of AI/wpbc.data\"\n",
        "data = pd.read_csv(path, header=None)\n",
        "\n",
        "column_names = [\n",
        "    \"id\", \"status\", \"time\",\n",
        "    \"mean_radius\", \"mean_texture\", \"mean_perimeter\", \"mean_area\",\n",
        "    \"mean_smoothness\", \"mean_compactness\", \"mean_concavity\",\n",
        "    \"mean_concave_points\", \"mean_symmetry\", \"mean_fractal_dimension\",\n",
        "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\",\n",
        "    \"smoothness_se\", \"compactness_se\", \"concavity_se\",\n",
        "    \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
        "    \"worst_radius\", \"worst_texture\", \"worst_perimeter\",\n",
        "    \"worst_area\", \"worst_smoothness\", \"worst_compactness\",\n",
        "    \"worst_concavity\", \"worst_concave_points\",\n",
        "    \"worst_symmetry\", \"worst_fractal_dimension\",\n",
        "    \"tumor_size\", \"lymph_nodes\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "vYc3ui3GSpNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic EDA\n",
        "print(\"===== SUMMARY STATISTICS =====\")\n",
        "print(data.describe())\n",
        "\n",
        "print(\"\\n===== MISSING VALUES =====\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Fill missing values for numeric columns only\n",
        "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n"
      ],
      "metadata": {
        "id": "ERkpNEBJWSXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target variable\n",
        "data[\"status\"] = data[\"status\"].map({\"N\": 0, \"R\": 1})\n",
        "\n",
        "# Drop ID column (not needed for prediction)\n",
        "X = data.drop([\"id\", \"status\"], axis=1)\n",
        "y = data[\"status\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "NpkDieRsW_NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Apply a Wrapper Method**\n",
        "\n",
        "1. Use Recursive Feature Elimination (RFE) with a Logistic Regression model to perform feature selection:\n",
        "\n",
        "• Select the top 5 features that contribute the most to predicting the target variable.\n",
        "\n",
        "• Visualize the ranking of features.\n",
        "\n",
        "2. Train the Logistic Regression model using only the selected features.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hguV3UAqXEb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace '?' with NaN and convert to float\n",
        "X_train = X_train.replace('?', np.nan).astype(float)\n",
        "X_test = X_test.replace('?', np.nan).astype(float)\n",
        "\n",
        "# Fill missing values with column mean\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Apply RFE to select top 5 features\n",
        "rfe = RFE(estimator=logreg, n_features_to_select=5)\n",
        "rfe.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature rankings\n",
        "feature_ranking = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Ranking': rfe.ranking_\n",
        "}).sort_values(by='Ranking')\n",
        "\n",
        "print(\"===== FEATURE RANKING =====\")\n",
        "print(feature_ranking)\n",
        "\n",
        "# Visualize feature rankings\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feature_ranking['Feature'], feature_ranking['Ranking'])\n",
        "plt.xlabel(\"Ranking (1 = Most Important)\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"RFE Feature Ranking\")\n",
        "plt.gca().invert_yaxis()  # most important on top\n",
        "plt.show()\n",
        "\n",
        "# List top 5 features\n",
        "top_features = feature_ranking[feature_ranking['Ranking'] == 1]['Feature'].tolist()\n",
        "print(\"\\nTop 5 selected features:\", top_features)\n"
      ],
      "metadata": {
        "id": "GRj597AtXKDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Select top features from train/test sets\n",
        "X_train_top = X_train[top_features]\n",
        "X_test_top = X_test[top_features]\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "logreg_top = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train model\n",
        "logreg_top.fit(X_train_top, y_train)\n",
        "\n",
        "# Check training and testing accuracy\n",
        "train_acc = logreg_top.score(X_train_top, y_train)\n",
        "test_acc = logreg_top.score(X_test_top, y_test)\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "2MUEvF52X01W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3: Model Evaluation**\n",
        "\n",
        "1. Evaluate the model’s performance using the testing set:\n",
        "\n",
        "• Metrics to calculate: Accuracy, Precision, Recall, F1-Score, and ROC-AUC.\n",
        "\n",
        "2. Compare the performance of the model trained on all features versus the model trained on the selected\n",
        "features.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PV--UjtYX6f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Model with all features\n",
        "logreg_all = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg_all.fit(X_train, y_train)\n",
        "y_pred_all = logreg_all.predict(X_test)\n",
        "y_prob_all = logreg_all.predict_proba(X_test)[:,1]  # probability for ROC-AUC\n",
        "\n",
        "# Model with top 5 features\n",
        "y_pred_top = logreg_top.predict(X_test_top)\n",
        "y_prob_top = logreg_top.predict_proba(X_test_top)[:,1]\n",
        "\n",
        "# Evaluation metrics\n",
        "def evaluate_model(y_true, y_pred, y_prob):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred),\n",
        "        \"Recall\": recall_score(y_true, y_pred),\n",
        "        \"F1-Score\": f1_score(y_true, y_pred),\n",
        "        \"ROC-AUC\": roc_auc_score(y_true, y_prob)\n",
        "    }\n",
        "\n",
        "# Evaluate both models\n",
        "metrics_all = evaluate_model(y_test, y_pred_all, y_prob_all)\n",
        "metrics_top = evaluate_model(y_test, y_pred_top, y_prob_top)\n",
        "\n",
        "# Print comparison\n",
        "print(\"===== MODEL PERFORMANCE =====\\n\")\n",
        "print(\"Model trained on ALL features:\")\n",
        "for k,v in metrics_all.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "print(\"\\nModel trained on TOP 5 features:\")\n",
        "for k,v in metrics_top.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "r2wlDPjZYAoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4: Experiment**\n",
        "1. Experiment with different numbers of selected features (e.g., top 3, top 7).\n",
        "2. Discuss how feature selection affects model performance.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lyy6mqDGYQ2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Numbers of features to experiment with\n",
        "feature_numbers = [3, 5, 7]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for n in feature_numbers:\n",
        "    # Apply RFE\n",
        "    rfe_exp = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "                  n_features_to_select=n)\n",
        "    rfe_exp.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Get top n features\n",
        "    top_n_features = X_train.columns[rfe_exp.support_].tolist()\n",
        "\n",
        "    # Train Logistic Regression on selected features\n",
        "    X_train_n = X_train[top_n_features]\n",
        "    X_test_n = X_test[top_n_features]\n",
        "    model_n = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model_n.fit(X_train_n, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_n = model_n.predict(X_test_n)\n",
        "    y_prob_n = model_n.predict_proba(X_test_n)[:,1]\n",
        "\n",
        "    # Evaluate\n",
        "    metrics_n = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred_n),\n",
        "        \"Precision\": precision_score(y_test, y_pred_n),\n",
        "        \"Recall\": recall_score(y_test, y_pred_n),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred_n),\n",
        "        \"ROC-AUC\": roc_auc_score(y_test, y_prob_n)\n",
        "    }\n",
        "\n",
        "    results[f\"Top {n} features\"] = {\"features\": top_n_features, \"metrics\": metrics_n}\n",
        "\n",
        "# Print results\n",
        "for key, value in results.items():\n",
        "    print(f\"\\n===== {key} =====\")\n",
        "    print(\"Selected Features:\", value[\"features\"])\n",
        "    print(\"Metrics:\")\n",
        "    for metric, score in value[\"metrics\"].items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "z6S5AV3tYbnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}